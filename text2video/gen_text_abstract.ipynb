{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*- \n",
    "import jieba  \n",
    "import jieba.posseg as pseg  \n",
    "import os  \n",
    "import sys\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import jieba  \\n  \\nseg_list = jieba.cut(\"\\xe6\\x88\\x91\\xe6\\x9d\\xa5\\xe5\\x88\\xb0\\xe5\\x8c\\x97\\xe4\\xba\\xac\\xe6\\xb8\\x85\\xe5\\x8d\\x8e\\xe5\\xa4\\xa7\\xe5\\xad\\xa6\", cut_all=True)#\\'generator\\' object has no attribute \\'__getitem__\\'\\n\\nprint(type(\"/ \".join(seg_list)))#<type \\'unicode\\'>\\n\\nprint(\"Full Mode: \" + \"/ \".join(seg_list))  # \\xe5\\x85\\xa8\\xe6\\xa8\\xa1\\xe5\\xbc\\x8f  '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import jieba  \n",
    "  \n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)#'generator' object has no attribute '__getitem__'\n",
    "\n",
    "print(type(\"/ \".join(seg_list)))#<type 'unicode'>\n",
    "\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式  '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tiyu', 'yule', 'jiaju', 'caipiao', 'fangchan', 'jiaoyu', 'shishang', 'shizheng', 'xingzuo', 'youxi', 'shehui', 'keji', 'gupiao', 'caijing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.316 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "1it [00:00,  3.03it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2it [00:00, 19.73it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#计算所有词的tfidf权值矩阵\n",
    "def find_files(directory, pattern='*.txt'):\n",
    "    '''Recursively finds all files matching the pattern.'''\n",
    "    filenames = []\n",
    "    for _,dirnames, files in os.walk(directory):\n",
    "        for file in fnmatch.filter(files, pattern):\n",
    "            filenames.append(file)\n",
    "    return filenames\n",
    "\n",
    "corpus = []\n",
    "\n",
    "path=\"./fasttext/THUCNews/\"\n",
    "files = os.listdir(path)  \n",
    "print files\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    filenames = find_files(\"./fasttext/THUCNews/\"+file)\n",
    "    \n",
    "    for i,filename in tqdm(enumerate(filenames)):\n",
    "        \n",
    "        path = \"./fasttext/THUCNews/\"+file+'/' + filename\n",
    "        with open(path, \"r\") as f:\n",
    "            \n",
    "            lines = f.read()\n",
    "\n",
    "\n",
    "            content = lines\n",
    "            #print 'content',content\n",
    "            if content != \"\":\n",
    "                \n",
    "                seg_content = jieba.cut(content)\n",
    "                seg_content = ' '.join(seg_content)\n",
    "\n",
    "                corpus.append(seg_content)\n",
    "        if i==5:\n",
    "            break\n",
    "#print 'corpus',len(corpus)\n",
    "vectorizer = CountVectorizer() # 该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频  \n",
    "transformer = TfidfTransformer() # 该类会统计每个词语的tf-idf权值 \n",
    "freq_matrix = vectorizer.fit_transform(corpus)\n",
    "tfidf = transformer.fit_transform(freq_matrix) # 第一个fit_transform是计算tf-idf，第二个fit_transform是将文本转为词频矩阵  \n",
    "word = vectorizer.get_feature_names() # 获取词袋模型中的所有词语\n",
    "tfidf = tfidf.tocoo()\n",
    "weight = tfidf.toarray()\n",
    "#print np.shape(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer() # 该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频  \n",
    "# transformer = TfidfTransformer() # 该类会统计每个词语的tf-idf权值 \n",
    "# freq_matrix = vectorizer.fit_transform(corpus)\n",
    "# tfidf = transformer.fit_transform(freq_matrix) # 第一个fit_transform是计算tf-idf，第二个fit_transform是将文本转为词频矩阵  \n",
    "# word = vectorizer.get_feature_names() # 获取词袋模型中的所有词语\n",
    "# tfidf = tfidf.tocoo()\n",
    "# #print tfidf\n",
    "# weight = tfidf.toarray() # 将tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in function where>\n"
     ]
    }
   ],
   "source": [
    "rows=[[1,2],[3,5]]\n",
    "print np.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练\n",
      "国奥队\n",
      "沈阳\n",
      "大雨\n",
      "冯萧霆\n",
      "感冒\n",
      "球员\n",
      "雨水\n",
      "长春\n",
      "没有\n",
      "load stopwords...\n",
      "texts: 马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军\n",
      "　　记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有摆脱雨水的困扰。7月31日下午6点，国奥队的日常训练再度受到大雨的干扰，无奈之下队员们只慢跑了25分钟就草草收场。\n",
      "　　31日上午10点，国奥队在奥体中心外场训练的时候，天就是阴沉沉的，气象预报显示当天下午沈阳就有大雨，但幸好队伍上午的训练并没有受到任何干扰。\n",
      "　　下午6点，当球队抵达训练场时，大雨已经下了几个小时，而且丝毫没有停下来的意思。抱着试一试的态度，球队开始了当天下午的例行训练，25分钟过去了，天气没有任何转好的迹象，为了保护球员们，国奥队决定中止当天的训练，全队立即返回酒店。\n",
      "　　在雨中训练对足球队来说并不是什么稀罕事，但在奥运会即将开始之前，全队变得“娇贵”了。在沈阳最后一周的训练，国奥队首先要保证现有的球员不再出现意外的伤病情况以免影响正式比赛，因此这一阶段控制训练受伤、控制感冒等疾病的出现被队伍放在了相当重要的位置。而抵达沈阳之后，中后卫冯萧霆就一直没有训练，冯萧霆是7月27日在长春患上了感冒，因此也没有参加29日跟塞尔维亚的热身赛。队伍介绍说，冯萧霆并没有出现发烧症状，但为了安全起见，这两天还是让他静养休息，等感冒彻底好了之后再恢复训练。由于有了冯萧霆这个例子，因此国奥队对雨中训练就显得特别谨慎，主要是担心球员们受凉而引发感冒，造成非战斗减员。而女足队员马晓旭在热身赛中受伤导致无缘奥运的前科，也让在沈阳的国奥队现在格外警惕，“训练中不断嘱咐队员们要注意动作，我们可不能再出这样的事情了。”一位工作人员表示。\n",
      "　　从长春到沈阳，雨水一路伴随着国奥队，“也邪了，我们走到哪儿雨就下到哪儿，在长春几次训练都被大雨给搅和了，没想到来沈阳又碰到这种事情。”一位国奥球员也对雨水的“青睐”有些不解。\n",
      "\n",
      "\n",
      "-----------approach 1-------------\n",
      "来到沈阳，国奥队依然没有摆脱雨水的困扰。\n",
      "而抵达沈阳之后，中后卫冯萧霆就一直没有训练，冯萧霆是7月27日在长春患上了感冒，因此也没有参加29日跟塞尔维亚的热身赛。\n",
      "　　从长春到沈阳，雨水一路伴随着国奥队，“也邪了，我们走到哪儿雨就下到哪儿，在长春几次训练都被大雨给搅和了，没想到来沈阳又碰到这种事情。\n",
      "-----------approach 2-------------\n",
      "来到沈阳，国奥队依然没有摆脱雨水的困扰。\n",
      "而抵达沈阳之后，中后卫冯萧霆就一直没有训练，冯萧霆是7月27日在长春患上了感冒，因此也没有参加29日跟塞尔维亚的热身赛。\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*- \n",
    "#文本摘要方法有很多，主要分为抽取式和生成式，应用比较多的是抽取式，也比较简单，就是从文本中抽取重要的句子或段落。本方法主要是利用句子中的关键词的距离，主要思想和参考来自阮一峰的网络日志http://www.ruanyifeng.com/blog/2013/03/automatic_summarization.html\n",
    "\n",
    "#!/user/bin/python\n",
    "# coding:utf-8\n",
    "__author__ = 'hj'\n",
    "import nltk\n",
    "import numpy\n",
    "import jieba\n",
    "import codecs\n",
    "#计算关键词\n",
    "import operator\n",
    "k = 0#第2个文件\n",
    "#print corpus[k]\n",
    "\n",
    "rows = tfidf.row\n",
    "#print type(rows),rows,len(rows)#每个词属于的文件\n",
    "cols = tfidf.col\n",
    "#print type(cols),cols,len(cols)#每个词的词号\n",
    "weights = tfidf.data\n",
    "row_index = np.where(rows==k)#第2个文件中的所有词在所有词中的位置 (1,390)\n",
    "#print np.shape(row_index[0]),type(row_index[0]),row_index[0]\n",
    "keywords = dict()\n",
    "for index in row_index[0]:\n",
    "    col_index = cols[index]#词号\n",
    "    keyword = word[col_index]#词\n",
    "    keywords[keyword] = weights[index]\n",
    "\n",
    "keywords = sorted(keywords.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#print keywords\n",
    "for keyword, weight in keywords[:10]:\n",
    "    print keyword\n",
    "    \n",
    "#抽取法提取文本摘要\n",
    "N=5#关键词数量\n",
    "CLUSTER_THRESHOLD=5#单词间的距离\n",
    "TOP_SENTENCES=2#返回的top n句子\n",
    "\n",
    "def find_files(directory, pattern='*.txt'):\n",
    "    '''Recursively finds all files matching the pattern.'''\n",
    "    filenames = []\n",
    "    for _,dirnames, files in os.walk(directory):\n",
    "        for file in fnmatch.filter(files, pattern):\n",
    "            filenames.append(file)\n",
    "    return filenames\n",
    "#分句\n",
    "def sent_tokenizer(path):\n",
    "    with open(path,'r')as f:\n",
    "        texts=f.read().decode('utf8')\n",
    "        print 'texts:',texts\n",
    "        \n",
    "        start=0\n",
    "        i=0#每个字符的位置\n",
    "        sentences=[]\n",
    "        punt_list='.!?。！？'.decode('utf8')+'\\n'+' ' #',.!?:;~，。！？：；～'.decode('utf8')\n",
    "        \n",
    "        for text in texts:\n",
    "            if text in punt_list and token not in punt_list: #检查标点符号下一个字符是否还是标点\n",
    "                sentences.append(texts[start:i+1])#当前标点符号位置\n",
    "                start=i+1#start标记到下一句的开头\n",
    "                i+=1\n",
    "            else:\n",
    "                i+=1#若不是标点符号，则字符位置继续前移\n",
    "                token=list(texts[start:i+2]).pop()#取下一个字符\n",
    "        if start<len(texts):\n",
    "            sentences.append(texts[start:])#这是为了处理文本末尾没有标点符号的情况\n",
    "    return sentences\n",
    "\n",
    "#停用词\n",
    "def load_stopwordslist(path):\n",
    "    print('load stopwords...')\n",
    "    stoplist=[line.strip() for line in codecs.open(path,'r',encoding='utf8').readlines()]\n",
    "    stopwrods={}.fromkeys(stoplist)\n",
    "    return stopwrods\n",
    "\n",
    "#摘要\n",
    "def summarize(text,keywords):\n",
    "    stopwords=load_stopwordslist('./Chinese/stopwords.dat')\n",
    "    sentences=sent_tokenizer(text)\n",
    "    #words=[w for sentence in sentences for w in jieba.cut(sentence) \n",
    "    #       if w not in stopwords if len(w)>1 and w!='\\t']\n",
    "    #wordfre=nltk.FreqDist(words)#每个单词的出现次数\n",
    "    #topn_words=[w[0] for w in sorted(wordfre.items(),key=lambda d:d[1],reverse=True)][:N]\n",
    "    #for i in range(len(topn_words)):\n",
    "    #    print topn_words[i].encode('utf8')\n",
    "    topn_words=[]\n",
    "    for keyword, weight in keywords[:10]:\n",
    "        topn_words.append(keyword)\n",
    "    scored_sentences=_score_sentences(sentences,topn_words)\n",
    "    #approach 1,利用均值和标准差过滤非重要句子\n",
    "    avg=numpy.mean([s[1] for s in scored_sentences])#均值\n",
    "    std=numpy.std([s[1] for s in scored_sentences])#标准差\n",
    "    mean_scored=[(sent_idx,score) for (sent_idx,score) in scored_sentences\n",
    "                 if score>(avg+0.5*std)]\n",
    "    #approach 2，返回top n句子\n",
    "    top_n_scored=sorted(scored_sentences,key=lambda s:s[1])[-TOP_SENTENCES:]\n",
    "    top_n_scored=sorted(top_n_scored,key=lambda s:s[0])\n",
    "    return dict(top_n_summary=[sentences[idx] for (idx,score) in top_n_scored],\n",
    "                mean_scored_summary=[sentences[idx] for (idx,score) in mean_scored])\n",
    "\n",
    " #句子得分\n",
    "def _score_sentences(sentences,topn_words):\n",
    "    scores=[]\n",
    "    sentence_idx=-1\n",
    "    for s in [list(jieba.cut(s)) for s in sentences]:\n",
    "        sentence_idx+=1\n",
    "        word_idx=[]\n",
    "        for w in topn_words:\n",
    "            try:\n",
    "                word_idx.append(s.index(w))#关键词出现在该句子中的索引位置\n",
    "            except ValueError:#w不在句子中\n",
    "                pass\n",
    "        word_idx.sort()\n",
    "        if len(word_idx)==0:\n",
    "            continue\n",
    "        #对于两个连续的单词，利用单词位置索引，通过距离阀值计算族\n",
    "        #一个句子划分为若干族\n",
    "        clusters=[]\n",
    "        cluster=[word_idx[0]]\n",
    "        i=1\n",
    "        while i<len(word_idx):\n",
    "            if word_idx[i]-word_idx[i-1]<CLUSTER_THRESHOLD:\n",
    "                cluster.append(word_idx[i])\n",
    "            else:\n",
    "                clusters.append(cluster[:])\n",
    "                cluster=[word_idx[i]]\n",
    "            i+=1\n",
    "        clusters.append(cluster)\n",
    "        #对每个族打分，每个族类的最大分数是对句子的打分\n",
    "        max_cluster_score=0\n",
    "        for c in clusters:\n",
    "            significant_words_in_cluster=len(c)\n",
    "            total_words_in_cluster=c[-1]-c[0]+1\n",
    "            score=1.0*significant_words_in_cluster*significant_words_in_cluster/total_words_in_cluster\n",
    "            if score>max_cluster_score:\n",
    "                max_cluster_score=score\n",
    "        scores.append((sentence_idx,max_cluster_score))\n",
    "    return scores;\n",
    "\n",
    "if __name__=='__main__':\n",
    "    dict=summarize('./fasttext/THUCNews/tiyu/0.txt',keywords)\n",
    "    print('-----------approach 1-------------')\n",
    "    for sent in dict['mean_scored_summary']:\n",
    "        print(sent)\n",
    "    print('-----------approach 2-------------')\n",
    "    for sent in dict['top_n_summary']:\n",
    "        print(sent)\n",
    "# if __name__=='__main__':\n",
    "#     dict=summarize(u'腾讯科技讯（刘亚澜）10月22日消息，'\n",
    "#         u'前优酷土豆技术副总裁黄冬已于日前正式加盟芒果TV，出任CTO一职。'\n",
    "#         u'资料显示，黄冬历任土豆网技术副总裁、优酷土豆集团产品技术副总裁等职务，'\n",
    "#         u'曾主持设计、运营过优酷土豆多个大型高容量产品和系统。'\n",
    "#         u'此番加入芒果TV或与芒果TV计划自主研发智能硬件OS有关。'\n",
    "#         u'今年3月，芒果TV对外公布其全平台日均独立用户突破3000万，日均VV突破1亿，'\n",
    "#         u'但挥之不去的是业内对其技术能力能否匹配发展速度的质疑，'\n",
    "#         u'亟须招揽技术人才提升整体技术能力。'\n",
    "#         u'芒果TV是国内互联网电视七大牌照方之一，之前采取的是“封闭模式”与硬件厂商预装合作，'\n",
    "#         u'而现在是“开放下载”+“厂商预装”。'\n",
    "#         u'黄冬在加盟土豆网之前曾是国内FreeBSD（开源OS）社区发起者之一，'\n",
    "#         u'是研究并使用开源OS的技术专家，离开优酷土豆集团后其加盟果壳电子，'\n",
    "#         u'涉足智能硬件行业，将开源OS与硬件结合，创办魔豆智能路由器。'\n",
    "#         u'未来黄冬可能会整合其在开源OS、智能硬件上的经验，结合芒果的牌照及资源优势，'\n",
    "#         u'在智能硬件或OS领域发力。'\n",
    "#         u'公开信息显示，芒果TV在今年6月对外宣布完成A轮5亿人民币融资，估值70亿。'\n",
    "#         u'据芒果TV控股方芒果传媒的消息人士透露，芒果TV即将启动B轮融资。')\n",
    "#     print('-----------approach 1-------------')\n",
    "#     for sent in dict['top_n_summary']:\n",
    "#         print(sent)\n",
    "#     print('-----------approach 2-------------')\n",
    "#     for sent in dict['mean_scored_summary']:\n",
    "#         print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
